<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Luna - Voice Assistant with Hand & Face Tracking</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            background-color: #1a1a2e;
            min-height: 100vh;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            color: #ffffff;
            display: flex;
            flex-direction: column;
            align-items: center;
            padding: 20px;
        }

        h1 {
            color: #7c83fd;
            margin-bottom: 20px;
            font-size: 24px;
        }

        .main-container {
            display: flex;
            gap: 30px;
            align-items: flex-start;
            flex-wrap: wrap;
            justify-content: center;
            max-width: 1200px;
        }

        .video-section {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 15px;
        }

        .video-container {
            position: relative;
            border-radius: 15px;
            overflow: hidden;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.5);
        }

        .video-container video {
            display: block;
            border-radius: 15px;
        }

        /* Luna face - smaller portrait */
        #luna-video {
            width: 240px;
            height: 320px;
            background-color: #1e1e28;
            object-fit: contain;
        }

        /* Camera preview - square */
        #camera-container {
            position: relative;
        }

        #camera-video {
            width: 320px;
            height: 240px;
            transform: scaleX(-1); /* Mirror the camera */
            background-color: #2a2a4e;
        }

        #face-overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 320px;
            height: 240px;
            pointer-events: none;
            transform: scaleX(-1); /* Match camera mirror */
        }

        .section-label {
            font-size: 14px;
            color: #7c83fd;
            text-align: center;
        }

        .controls-section {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
            min-width: 250px;
        }

        .status {
            padding: 10px 20px;
            background-color: #2a2a4e;
            border-radius: 10px;
            font-size: 14px;
            min-width: 200px;
            text-align: center;
        }

        .status.connected {
            background-color: #1e5631;
        }

        .status.error {
            background-color: #5e1e1e;
        }

        .connect-btn {
            padding: 15px 40px;
            font-size: 18px;
            background-color: #7c83fd;
            color: white;
            border: none;
            border-radius: 30px;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .connect-btn:hover {
            background-color: #6a71e5;
            transform: scale(1.05);
        }

        .connect-btn:disabled {
            background-color: #4a4a6a;
            cursor: not-allowed;
            transform: none;
        }

        .disconnect-btn {
            background-color: #e74c3c;
        }

        .disconnect-btn:hover {
            background-color: #c0392b;
        }

        .face-tracking-status {
            padding: 8px 15px;
            background-color: #2a2a4e;
            border-radius: 8px;
            font-size: 12px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .tracking-dot {
            width: 10px;
            height: 10px;
            border-radius: 50%;
            background-color: #666;
        }

        .tracking-dot.active {
            background-color: #4CAF50;
            animation: pulse 1s infinite;
        }

        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }

        .transcript {
            width: 100%;
            max-width: 800px;
            margin-top: 30px;
            padding: 20px;
            background-color: #2a2a4e;
            border-radius: 15px;
            max-height: 300px;
            overflow-y: auto;
        }

        .transcript h3 {
            margin-bottom: 15px;
            color: #7c83fd;
            font-size: 16px;
        }

        .transcript-content {
            font-size: 14px;
            line-height: 1.8;
            display: flex;
            flex-direction: column;
            gap: 12px;
        }

        .transcript-line {
            padding: 10px 14px;
            border-radius: 12px;
            max-width: 85%;
            word-wrap: break-word;
        }

        .transcript-line.user {
            background-color: #4a4a7e;
            align-self: flex-end;
            border-bottom-right-radius: 4px;
        }

        .transcript-line.assistant {
            background-color: #3a3a6e;
            align-self: flex-start;
            border-bottom-left-radius: 4px;
        }

        .transcript-line.streaming {
            opacity: 0.9;
        }

        .transcript-line.streaming::after {
            content: 'â–‹';
            animation: blink 1s infinite;
            margin-left: 2px;
        }

        @keyframes blink {
            0%, 50% { opacity: 1; }
            51%, 100% { opacity: 0; }
        }

        .transcript-label {
            font-size: 11px;
            color: #8888aa;
            margin-bottom: 4px;
            font-weight: 500;
        }

        .speaking-indicator {
            display: none;
            padding: 10px 14px;
            background-color: #3a3a6e;
            border-radius: 12px;
            align-self: flex-start;
            color: #8888cc;
            font-style: italic;
        }

        .speaking-indicator.active {
            display: block;
        }

        .speaking-indicator::after {
            content: '...';
            animation: dots 1.5s infinite;
        }

        @keyframes dots {
            0%, 20% { content: '.'; }
            40% { content: '..'; }
            60%, 100% { content: '...'; }
        }

        .gaze-debug {
            margin-top: 10px;
            font-size: 12px;
            color: #888;
        }
    </style>
</head>
<body>
    <h1>Luna - Voice Assistant</h1>

    <div class="main-container">
        <!-- Luna's Face (from backend) -->
        <div class="video-section">
            <div class="video-container">
                <video id="luna-video" autoplay playsinline></video>
            </div>
            <span class="section-label">Luna</span>
        </div>

        <!-- Controls -->
        <div class="controls-section">
            <div class="status" id="status">Ready to connect</div>

            <button class="connect-btn" id="connect-btn">
                Start Conversation
            </button>

            <div class="face-tracking-status">
                <div class="tracking-dot" id="tracking-dot"></div>
                <span id="tracking-status">Face tracking: Loading...</span>
            </div>

            <div class="gaze-debug" id="gaze-debug">Gaze: -</div>
        </div>

        <!-- Camera with Face Detection -->
        <div class="video-section">
            <div class="video-container" id="camera-container">
                <video id="camera-video" autoplay playsinline muted></video>
                <canvas id="face-overlay"></canvas>
            </div>
            <span class="section-label" id="camera-label">Your Camera (Face Tracking)</span>
        </div>
    </div>

    <div class="transcript">
        <h3>Conversation</h3>
        <div class="transcript-content" id="transcript">
            <div class="speaking-indicator" id="user-speaking">Listening</div>
        </div>
    </div>

    <!-- Hidden audio element for bot voice -->
    <audio id="bot-audio" autoplay></audio>

    <script type="module">
        // Import MediaPipe Face Detection and Hand Landmarker
        import { FilesetResolver, FaceDetector, HandLandmarker } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest";

        // State
        let peerConnection = null;
        let dataChannel = null;
        let isConnected = false;
        let cameraStream = null;
        let faceDetector = null;
        let handLandmarker = null;
        let isTracking = false;
        let lastGazeUpdate = 0;
        let lastVideoTime = -1;
        let trackingSource = 'none'; // 'hand', 'face', or 'none'
        let frameCount = 0;
        let lastDetectionTime = 0;
        // Detection intervals (ms) - higher = less CPU
        const FACE_DETECTION_INTERVAL_MS = 150;  // Face: ~7 FPS
        const HAND_DETECTION_INTERVAL_MS = 400;  // Hand: ~2.5 FPS (much heavier model)
        let lastFaceDetectionTime = 0;
        let lastHandDetectionTime = 0;
        // Sticky tracking - remember last known positions to avoid flashing
        let lastHandGaze = null;  // {x, y} or null if no hand
        let lastFaceGaze = null;  // {x, y} or null if no face
        let handVisible = false;  // Is hand currently detected?
        // Set to true to completely disable hand tracking
        const DISABLE_HAND_TRACKING = false;

        // DOM Elements
        const statusEl = document.getElementById('status');
        const connectBtn = document.getElementById('connect-btn');
        const transcriptEl = document.getElementById('transcript');
        const lunaVideo = document.getElementById('luna-video');
        const cameraVideo = document.getElementById('camera-video');
        const faceOverlay = document.getElementById('face-overlay');
        const trackingDot = document.getElementById('tracking-dot');
        const trackingStatus = document.getElementById('tracking-status');
        const gazeDebug = document.getElementById('gaze-debug');
        const botAudio = document.getElementById('bot-audio');
        const overlayCtx = faceOverlay.getContext('2d');

        // State for transcript handling
        let currentBotMessage = null;
        let botTextBuffer = '';
        let userSpeakingIndicator = null;

        function setStatus(text, type = '') {
            statusEl.textContent = text;
            statusEl.className = 'status ' + type;
        }

        function addTranscript(text, role, isStreaming = false) {
            const line = document.createElement('div');
            line.className = 'transcript-line ' + role + (isStreaming ? ' streaming' : '');

            const label = document.createElement('div');
            label.className = 'transcript-label';
            label.textContent = role === 'user' ? 'You' : 'Luna';

            const content = document.createElement('div');
            content.textContent = text;

            line.appendChild(label);
            line.appendChild(content);

            // Insert before the speaking indicator if it exists
            const speakingIndicator = document.getElementById('user-speaking');
            if (speakingIndicator) {
                transcriptEl.insertBefore(line, speakingIndicator);
            } else {
                transcriptEl.appendChild(line);
            }
            scrollToBottom();
            return line;
        }

        function scrollToBottom() {
            // Use smooth scroll to keep newest content visible
            const transcriptContainer = document.querySelector('.transcript');
            transcriptContainer.scrollTo({
                top: transcriptContainer.scrollHeight,
                behavior: 'smooth'
            });
        }

        function startBotMessage() {
            // Only reset if there's no message currently in progress
            // This prevents clearing the buffer when tool calls trigger multiple bot-llm-started events
            if (!currentBotMessage) {
                botTextBuffer = '';
            }
        }

        function appendBotText(text) {
            // Only create the message element when we have actual text
            if (!currentBotMessage && text.trim()) {
                currentBotMessage = addTranscript('', 'assistant', true);
            }
            if (currentBotMessage) {
                botTextBuffer += text;
                const contentEl = currentBotMessage.querySelector('div:last-child');
                if (contentEl) {
                    contentEl.textContent = botTextBuffer;
                }
                scrollToBottom();
            }
        }

        function finishBotMessage() {
            if (currentBotMessage) {
                currentBotMessage.classList.remove('streaming');
            }
            currentBotMessage = null;
            botTextBuffer = '';
        }

        function setUserSpeaking(speaking) {
            const indicator = document.getElementById('user-speaking');
            if (indicator) {
                indicator.classList.toggle('active', speaking);
                if (speaking) {
                    scrollToBottom();
                }
            }
        }

        // ============== MediaPipe Hand & Face Detection ==============
        async function initTracking() {
            try {
                trackingStatus.textContent = 'Tracking: Initializing...';

                // Start camera first
                cameraStream = await navigator.mediaDevices.getUserMedia({
                    video: { width: 640, height: 480, facingMode: 'user' },
                    audio: false
                });
                cameraVideo.srcObject = cameraStream;

                // Wait for video to be ready
                await new Promise((resolve) => {
                    cameraVideo.onloadedmetadata = () => {
                        cameraVideo.play();
                        resolve();
                    };
                });

                // Set up canvas size
                faceOverlay.width = 320;
                faceOverlay.height = 240;

                // Initialize MediaPipe Vision (shared between face and hand detection)
                const vision = await FilesetResolver.forVisionTasks(
                    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
                );

                // Initialize Hand Landmarker (priority) - skip if disabled for Pi
                if (!DISABLE_HAND_TRACKING) {
                    handLandmarker = await HandLandmarker.createFromOptions(vision, {
                        baseOptions: {
                            modelAssetPath: "https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task",
                            delegate: "GPU"
                        },
                        runningMode: "VIDEO",
                        numHands: 1,
                        minHandDetectionConfidence: 0.5,
                        minHandPresenceConfidence: 0.5,
                        minTrackingConfidence: 0.5
                    });
                }

                // Initialize Face Detector
                faceDetector = await FaceDetector.createFromOptions(vision, {
                    baseOptions: {
                        modelAssetPath: "https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite",
                        delegate: "GPU"
                    },
                    runningMode: "VIDEO",
                    minDetectionConfidence: 0.5
                });

                // Start detection loop
                isTracking = true;
                trackingDot.classList.add('active');
                trackingStatus.textContent = DISABLE_HAND_TRACKING
                    ? 'Tracking: Face Only (Pi mode)'
                    : 'Tracking: Hand > Face';
                runDetection();

                console.log(DISABLE_HAND_TRACKING ? 'Face detection initialized (Pi mode)' : 'Hand & Face detection initialized');
            } catch (error) {
                console.error('Failed to init tracking:', error);
                trackingStatus.textContent = 'Tracking: Error - ' + error.message;
            }
        }

        // Main detection loop - uses SEPARATE throttling for hand vs face
        // Hand detection is heavy (~50% CPU) so runs less often
        // Face detection is lighter, runs more frequently
        function runDetection() {
            if (!isTracking) {
                requestAnimationFrame(runDetection);
                return;
            }

            // Only process when we have a new video frame
            if (cameraVideo.readyState < 2 || cameraVideo.currentTime === lastVideoTime) {
                requestAnimationFrame(runDetection);
                return;
            }

            lastVideoTime = cameraVideo.currentTime;
            const now = performance.now();
            const startTimeMs = now;

            try {
                // Clear overlay
                overlayCtx.clearRect(0, 0, faceOverlay.width, faceOverlay.height);

                // Check if it's time for hand detection (runs less often - expensive)
                const shouldRunHand = handLandmarker && (now - lastHandDetectionTime >= HAND_DETECTION_INTERVAL_MS);

                // Check if it's time for face detection (runs more often - cheaper)
                const shouldRunFace = faceDetector && (now - lastFaceDetectionTime >= FACE_DETECTION_INTERVAL_MS);

                // 1. Run hand detection if scheduled (updates handVisible and lastHandGaze)
                if (shouldRunHand) {
                    lastHandDetectionTime = now;
                    const handResults = handLandmarker.detectForVideo(cameraVideo, startTimeMs);
                    if (handResults.landmarks && handResults.landmarks.length > 0) {
                        const landmarks = handResults.landmarks[0];
                        const wrist = landmarks[0];
                        const middleMCP = landmarks[9];
                        const handCenterX = (wrist.x + middleMCP.x) / 2;
                        const handCenterY = (wrist.y + middleMCP.y) / 2;

                        lastHandGaze = { x: 1 - handCenterX, y: handCenterY };
                        handVisible = true;
                        drawHandLandmarks(landmarks);
                    } else {
                        // Hand detection ran but found no hand
                        handVisible = false;
                        lastHandGaze = null;
                    }
                }

                // 2. Run face detection if scheduled (updates lastFaceGaze)
                if (shouldRunFace) {
                    lastFaceDetectionTime = now;
                    const faceResults = faceDetector.detectForVideo(cameraVideo, startTimeMs);
                    if (faceResults.detections && faceResults.detections.length > 0) {
                        let largestFace = faceResults.detections[0];
                        let largestArea = 0;
                        for (const detection of faceResults.detections) {
                            const bbox = detection.boundingBox;
                            const area = bbox.width * bbox.height;
                            if (area > largestArea) {
                                largestArea = area;
                                largestFace = detection;
                            }
                        }

                        const bbox = largestFace.boundingBox;
                        const normalizedX = (bbox.originX + bbox.width / 2) / cameraVideo.videoWidth;
                        const normalizedY = (bbox.originY + bbox.height / 2) / cameraVideo.videoHeight;
                        lastFaceGaze = { x: 1 - normalizedX, y: normalizedY };

                        // Only draw face box if hand is NOT visible
                        if (!handVisible) {
                            drawFaceBoundingBox(bbox);
                        }
                    } else {
                        lastFaceGaze = null;
                    }
                }

                // 3. Decide what gaze to use: HAND has priority, then FACE
                // Use sticky values - don't switch between detections
                let gazeX = null;
                let gazeY = null;

                if (handVisible && lastHandGaze) {
                    // Hand is currently visible - use hand gaze
                    gazeX = lastHandGaze.x;
                    gazeY = lastHandGaze.y;
                    trackingSource = 'hand';
                } else if (lastFaceGaze) {
                    // No hand visible - use face gaze
                    gazeX = lastFaceGaze.x;
                    gazeY = lastFaceGaze.y;
                    trackingSource = 'face';
                }

                // 4. Update debug and send gaze if we have tracking
                if (gazeX !== null) {
                    const sourceLabel = trackingSource === 'hand' ? 'Hand' : 'Face';
                    gazeDebug.textContent = `${sourceLabel}: X=${gazeX.toFixed(2)}, Y=${gazeY.toFixed(2)}`;

                    // Send gaze to backend (throttle to 10 updates/sec)
                    const currentTime = Date.now();
                    if (dataChannel && dataChannel.readyState === 'open' && currentTime - lastGazeUpdate > 100) {
                        dataChannel.send(JSON.stringify({
                            type: 'gaze',
                            x: gazeX,
                            y: gazeY
                        }));
                        lastGazeUpdate = currentTime;
                    }
                } else {
                    gazeDebug.textContent = 'No hand or face detected';
                    trackingSource = 'none';
                }
            } catch (e) {
                console.error('Detection error:', e);
            }

            requestAnimationFrame(runDetection);
        }

        // Draw simplified hand landmarks (palm outline + finger tips)
        function drawHandLandmarks(landmarks) {
            const scaleX = faceOverlay.width / cameraVideo.videoWidth;
            const scaleY = faceOverlay.height / cameraVideo.videoHeight;

            // Palm connections (simplified)
            const palmIndices = [0, 1, 5, 9, 13, 17, 0];  // Wrist to each finger base, back to wrist

            overlayCtx.strokeStyle = '#4CAF50';  // Green for hand
            overlayCtx.lineWidth = 2;
            overlayCtx.beginPath();

            for (let i = 0; i < palmIndices.length; i++) {
                const landmark = landmarks[palmIndices[i]];
                const x = landmark.x * cameraVideo.videoWidth * scaleX;
                const y = landmark.y * cameraVideo.videoHeight * scaleY;
                if (i === 0) {
                    overlayCtx.moveTo(x, y);
                } else {
                    overlayCtx.lineTo(x, y);
                }
            }
            overlayCtx.stroke();

            // Draw center point (between wrist and middle finger)
            const wrist = landmarks[0];
            const middleMCP = landmarks[9];
            const centerX = ((wrist.x + middleMCP.x) / 2) * cameraVideo.videoWidth * scaleX;
            const centerY = ((wrist.y + middleMCP.y) / 2) * cameraVideo.videoHeight * scaleY;

            overlayCtx.fillStyle = '#4CAF50';
            overlayCtx.beginPath();
            overlayCtx.arc(centerX, centerY, 6, 0, Math.PI * 2);
            overlayCtx.fill();
        }

        // Draw face bounding box
        function drawFaceBoundingBox(bbox) {
            const scaleX = faceOverlay.width / cameraVideo.videoWidth;
            const scaleY = faceOverlay.height / cameraVideo.videoHeight;

            const x = bbox.originX * scaleX;
            const y = bbox.originY * scaleY;
            const w = bbox.width * scaleX;
            const h = bbox.height * scaleY;

            // Draw bounding box
            overlayCtx.strokeStyle = '#7c83fd';  // Blue for face
            overlayCtx.lineWidth = 3;
            overlayCtx.strokeRect(x, y, w, h);

            // Draw center dot
            const centerX = x + w / 2;
            const centerY = y + h / 2;
            overlayCtx.fillStyle = '#7c83fd';
            overlayCtx.beginPath();
            overlayCtx.arc(centerX, centerY, 5, 0, Math.PI * 2);
            overlayCtx.fill();
        }

        // ============== WebRTC Connection ==============
        async function toggleConnection() {
            if (isConnected) {
                disconnect();
            } else {
                await connect();
            }
        }

        async function connect() {
            try {
                setStatus('Starting session...', '');
                connectBtn.disabled = true;

                // 1. Start session
                const startResponse = await fetch('/start', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ enableDefaultIceServers: true })
                });
                const startData = await startResponse.json();
                const sessionId = startData.sessionId;

                setStatus('Creating connection...', '');

                // 2. Create RTCPeerConnection
                const config = startData.iceConfig || { iceServers: [{ urls: 'stun:stun.l.google.com:19302' }] };
                peerConnection = new RTCPeerConnection(config);

                // 3. Get user audio
                const audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioStream.getTracks().forEach(track => peerConnection.addTrack(track, audioStream));

                // 3b. Add transceiver for VIDEO from server
                // Important: Use 'sendrecv' direction so server can attach its video track
                const videoTransceiver = peerConnection.addTransceiver('video', { direction: 'sendrecv' });
                console.log('Added video transceiver:', videoTransceiver);

                // 4. Handle incoming tracks (audio + video from Luna)
                peerConnection.ontrack = (event) => {
                    console.log('Got track:', event.track.kind, 'streams:', event.streams.length);
                    if (event.track.kind === 'video') {
                        console.log('Setting video stream to luna-video element');
                        lunaVideo.srcObject = event.streams[0];
                        // Also try playing explicitly
                        lunaVideo.play().catch(e => console.log('Video play error:', e));
                    } else if (event.track.kind === 'audio') {
                        botAudio.srcObject = event.streams[0];
                    }
                };

                // 5. Create data channel for RTVI messages
                dataChannel = peerConnection.createDataChannel('rtvi');
                dataChannel.onopen = () => {
                    console.log('Data channel open');
                    dataChannel.send(JSON.stringify({ type: 'client-ready' }));
                };
                dataChannel.onmessage = (event) => {
                    handleRTVIMessage(JSON.parse(event.data));
                };

                // 6. Create offer
                const offer = await peerConnection.createOffer();
                await peerConnection.setLocalDescription(offer);

                // Wait for ICE gathering
                await new Promise((resolve) => {
                    if (peerConnection.iceGatheringState === 'complete') {
                        resolve();
                    } else {
                        peerConnection.addEventListener('icegatheringstatechange', () => {
                            if (peerConnection.iceGatheringState === 'complete') {
                                resolve();
                            }
                        });
                    }
                });

                setStatus('Connecting to Luna...', '');

                // 7. Send offer to server
                const offerResponse = await fetch(`/sessions/${sessionId}/api/offer`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        sdp: peerConnection.localDescription.sdp,
                        type: peerConnection.localDescription.type
                    })
                });
                const answerData = await offerResponse.json();

                // 8. Set remote description
                await peerConnection.setRemoteDescription(new RTCSessionDescription({
                    type: 'answer',
                    sdp: answerData.sdp
                }));

                isConnected = true;
                setStatus('Connected to Luna!', 'connected');
                connectBtn.textContent = 'End Conversation';
                connectBtn.classList.add('disconnect-btn');
                connectBtn.disabled = false;

            } catch (error) {
                console.error('Connection error:', error);
                setStatus('Connection failed: ' + error.message, 'error');
                connectBtn.disabled = false;
                disconnect();
            }
        }

        function disconnect() {
            if (peerConnection) {
                peerConnection.close();
                peerConnection = null;
            }
            if (dataChannel) {
                dataChannel.close();
                dataChannel = null;
            }
            isConnected = false;
            setStatus('Disconnected', '');
            connectBtn.textContent = 'Start Conversation';
            connectBtn.classList.remove('disconnect-btn');
            connectBtn.disabled = false;

            // Clear Luna video
            lunaVideo.srcObject = null;
        }

        function handleRTVIMessage(message) {
            // Only log important messages for debugging
            const importantTypes = ['bot-ready', 'error', 'user-transcription', 'bot-llm-started', 'bot-llm-stopped'];
            if (importantTypes.includes(message.type)) {
                console.log('RTVI:', message.type, message.data || '');
            }

            switch (message.type) {
                // User speaking indicators
                case 'user-started-speaking':
                    setUserSpeaking(true);
                    break;

                case 'user-stopped-speaking':
                    setUserSpeaking(false);
                    break;

                // User final transcription
                case 'user-transcription':
                    if (message.data && message.data.text) {
                        setUserSpeaking(false);
                        addTranscript(message.data.text, 'user');
                    }
                    break;

                // Bot starts generating (LLM)
                case 'bot-llm-started':
                    startBotMessage();
                    break;

                // Bot LLM text (streaming words)
                case 'bot-llm-text':
                    if (message.data && message.data.text) {
                        appendBotText(message.data.text);
                    }
                    break;

                // Bot stops generating
                case 'bot-llm-stopped':
                    finishBotMessage();
                    break;

                // TTS text - ignore since we use bot-llm-text for streaming
                // (bot-tts-text sends per-sentence which causes duplicates)
                case 'bot-tts-text':
                    // Ignored - using bot-llm-text instead
                    break;

                // Bot ready
                case 'bot-ready':
                    console.log('Bot is ready');
                    break;
            }
        }

        // ============== Initialization ==============
        // Attach button click handler (needed because we're using ES modules)
        connectBtn.addEventListener('click', toggleConnection);

        window.addEventListener('load', async () => {
            // Start hand + face detection immediately (for preview)
            await initTracking();
        });

        // Cleanup on page unload
        window.addEventListener('beforeunload', () => {
            isTracking = false;
            disconnect();
            if (cameraStream) {
                cameraStream.getTracks().forEach(track => track.stop());
            }
        });
    </script>
</body>
</html>
